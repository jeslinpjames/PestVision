{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jesli\\anaconda3\\envs\\pv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data_utils.foreground_loader import IP102Dataset, ForegroundBlur, ForegroundRotate\n",
    "from data_utils.background_loader import PaddyDiseaseClassificationDataset, RiceLeafsDataset, BackgroundRandomCrop\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IP102 train size: 12104\n",
      "IP102 val size: 3069\n",
      "IP102 test size: 3796\n"
     ]
    }
   ],
   "source": [
    "ip102_dir = 'D:/git/pestvision_data/pestvision_data/foreground_data/Detection_IP102' # change this path appropriately\n",
    "source_range_big = (40, 60)\n",
    "source_range_small = (60, 80)\n",
    "transform_BlurRotate = T.Compose(\n",
    "    [ForegroundBlur(blur_prob=0.35), ForegroundRotate(rotation_prob=0.35)]\n",
    ")\n",
    "\n",
    "ip102_dataset_train = IP102Dataset(dataset_dir= ip102_dir,\n",
    "                                   split='train',\n",
    "                                   source_image_range_big=source_range_big,\n",
    "                                   source_image_range_small=source_range_small,\n",
    "                                   transform=transform_BlurRotate)\n",
    "\n",
    "ip102_dataset_val = IP102Dataset(dataset_dir= ip102_dir,\n",
    "                                   split='val',\n",
    "                                   source_image_range_big=source_range_big,\n",
    "                                   source_image_range_small=source_range_small,\n",
    "                                   transform=transform_BlurRotate)\n",
    "\n",
    "ip102_dataset_test = IP102Dataset(dataset_dir= ip102_dir,\n",
    "                                   split='test',\n",
    "                                   source_image_range_big=source_range_big,\n",
    "                                   source_image_range_small=source_range_small,\n",
    "                                   transform=transform_BlurRotate)\n",
    "\n",
    "print(f'IP102 train size: {len(ip102_dataset_train)}')\n",
    "print(f'IP102 val size: {len(ip102_dataset_val)}')\n",
    "print(f'IP102 test size: {len(ip102_dataset_test)}')\n",
    "\n",
    "foreground_datasets = {'train': ip102_dataset_train, 'val': ip102_dataset_val, 'test': ip102_dataset_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paddy Disease Classification train size: 6762\n",
      "Paddy Disease Classification val size: 1559\n",
      "Paddy Disease Classification test size: 2086\n"
     ]
    }
   ],
   "source": [
    "paddy_disease_classification_dir = 'D:/git/pestvision_data/pestvision_data/background_data/paddy_disease_classification' # change this path appropriately\n",
    "background_transform = BackgroundRandomCrop(crop_prob=0.6)\n",
    "\n",
    "paddy_disease_classification_dataset_train= PaddyDiseaseClassificationDataset(dataset_dir=paddy_disease_classification_dir, \n",
    "                                                                              split=\"train\", transform=background_transform,\n",
    "                                                                              target_size=512)\n",
    "paddy_disease_classification_dataset_val= PaddyDiseaseClassificationDataset(dataset_dir=paddy_disease_classification_dir,\n",
    "                                                                            split=\"val\", transform=background_transform,\n",
    "                                                                            target_size=512)\n",
    "paddy_disease_classification_dataset_test= PaddyDiseaseClassificationDataset(dataset_dir=paddy_disease_classification_dir,\n",
    "                                                                             split=\"test\", transform=background_transform,\n",
    "                                                                             target_size=512)\n",
    "\n",
    "print(f'Paddy Disease Classification train size: {len(paddy_disease_classification_dataset_train)}')\n",
    "print(f'Paddy Disease Classification val size: {len(paddy_disease_classification_dataset_val)}')\n",
    "print(f'Paddy Disease Classification test size: {len(paddy_disease_classification_dataset_test)}')\n",
    "\n",
    "background_datasets = {'train': paddy_disease_classification_dataset_train, 'val': paddy_disease_classification_dataset_val, 'test': paddy_disease_classification_dataset_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [03:11<00:00, 191.44s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "from skimage.io import imsave\n",
    "from utils_deep_image_blending import compute_gt_gradient, make_canvas_mask, numpy2tensor, laplacian_filter_tensor, MeanShift, Vgg16, gram_matrix\n",
    "import os\n",
    "from data_utils.foreground_loader import AbstractForegroundPestDataset\n",
    "from data_utils.background_loader import AbstractBackgroundDataset\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple\n",
    "\n",
    "# Abstract class for synthetic data generation: pest blending\n",
    "# Concrete classes for different pest blending methods\n",
    "\n",
    "class AbstractPestBlending(ABC):\n",
    "\n",
    "    def __init__(self,\n",
    "                 outputImagesDir: str,\n",
    "                 outputLabelsDir: str,\n",
    "                 outputMetadataDir: str,\n",
    "                 device: str,\n",
    "                 max_pests_per_image: int):\n",
    "\n",
    "        self.outputImagesDir = outputImagesDir\n",
    "        self.outputLabelsDir = outputLabelsDir\n",
    "        self.outputMetadataDir = outputMetadataDir\n",
    "        self.device = device\n",
    "        self.max_pests = max_pests_per_image\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_blended_image(self,\n",
    "                               foreground_dataset: AbstractForegroundPestDataset,\n",
    "                               N_foreground: int,\n",
    "                               background_dataset: AbstractBackgroundDataset,\n",
    "                               N_background: int,\n",
    "                               split: str,\n",
    "                               file_save_index: int):\n",
    "        pass\n",
    "\n",
    "\n",
    "class DeepImageBlending(AbstractPestBlending):\n",
    "\n",
    "    def __init__(self,\n",
    "                 outputImagesDir: str,\n",
    "                 outputLabelsDir: str,\n",
    "                 outputMetadataDir: str,\n",
    "                 device: str,\n",
    "                 max_pests_per_image: int = 10,\n",
    "                 target_image_size: int = 512,\n",
    "                 source_image_range_big: Tuple[int, int] = (40, 60),\n",
    "                 source_image_range_small: Tuple[int, int] = (60, 80),\n",
    "                 num_steps1: int = 1000):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "\n",
    "            outputImagesDir (str): path to the directory where the output images will be saved.\n",
    "            outputLabelsDir (str): path to the directory where the output labels will be saved.\n",
    "            outputMetadataDir (str): path to the directory where the output metadata will be saved.\n",
    "            device (str): device to run the model on.\n",
    "            max_pests_per_image (int): maximum number of pests per image.\n",
    "            target_image_size (int): size of the target image.\n",
    "            source_image_range_big (Tuple[int, int]): range for big source images.\n",
    "            source_image_range_small (Tuple[int, int]): range for small source images.\n",
    "            num_steps1 (int): number of steps for the first pass of deep image blending.\n",
    "            (Note: num_steps2 = 0)\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(outputImagesDir, outputLabelsDir,\n",
    "                         outputMetadataDir, device, max_pests_per_image)\n",
    "\n",
    "        self.source_image_range_small = source_image_range_small\n",
    "        self.source_image_range_big = source_image_range_big\n",
    "        self.ts = target_image_size\n",
    "        self.num_steps1 = num_steps1\n",
    "\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            os.makedirs(os.path.join(self.outputImagesDir, split), exist_ok=True)\n",
    "            os.makedirs(os.path.join(self.outputLabelsDir, split), exist_ok=True)\n",
    "            os.makedirs(os.path.join(self.outputMetadataDir, split), exist_ok=True)\n",
    "\n",
    "    def generate_blended_image(self,\n",
    "                               foreground_dataset: AbstractForegroundPestDataset,\n",
    "                               N_foreground: int,\n",
    "                               background_dataset: AbstractBackgroundDataset,\n",
    "                               N_background: int,\n",
    "                               split: str,\n",
    "                               file_save_index: int):\n",
    "        \"\"\"\n",
    "        Generate blended image using deep image blending.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "                foreground_dataset (AbstractForegroundPestDataset): dataset of foreground images.\n",
    "                N_foreground (int): number of samples in the foreground dataset.\n",
    "                background_dataset (AbstractBackgroundDataset): dataset of background images.\n",
    "                N_background (int): number of samples in the background dataset.\n",
    "                split (str): split of the dataset.\n",
    "                file_save_index (int): index to save the file.\n",
    "        \"\"\"\n",
    "\n",
    "        background_sample = background_dataset[np.random.randint(N_background)]\n",
    "        target_img = np.array(background_sample['image'])\n",
    "        target_filename = background_sample['image_filename']\n",
    "        no_pests = np.random.randint(low=0, high=self.max_pests + 1)\n",
    "\n",
    "        if no_pests == 0:  # no labels or metadata stored\n",
    "\n",
    "            no_pest_img_file = os.path.join(\n",
    "                self.outputImagesDir, split, f'{file_save_index}.png')\n",
    "            imsave(no_pest_img_file, target_img.astype(np.uint8))\n",
    "\n",
    "            str_metadata = f\"{file_save_index}.png {0}/{no_pests} {target_filename}\\n\"\n",
    "\n",
    "            metadata_path = os.path.join(\n",
    "                self.outputMetadataDir, split, \"metadata.txt\")\n",
    "            with open(metadata_path, 'a') as f:\n",
    "                f.write(str_metadata)\n",
    "\n",
    "        centers = np.random.randint(\n",
    "            low=self.source_image_range_small[1], high=self.ts - self.source_image_range_small[1], size=(no_pests, 2))\n",
    "\n",
    "        for j in tqdm(range(no_pests), desc=\" Iterating over no_pests\", leave=False):\n",
    "\n",
    "            foreground_sample = foreground_dataset[np.random.randint(N_foreground)]\n",
    "\n",
    "            source_img = np.array(foreground_sample[\"source_img_resized\"])\n",
    "            source_filename = foreground_sample['source_filename']\n",
    "            mask_img = np.array(foreground_sample[\"mask_img_resized\"])\n",
    "            mask_img[mask_img > 0] = 1\n",
    "            pest_class_id = foreground_sample[\"pest_class_id\"]\n",
    "            ss = foreground_sample[\"source_size\"]\n",
    "            is_big = foreground_sample[\"is_big\"]\n",
    "\n",
    "            x_start = centers[j][0]\n",
    "            y_start = centers[j][1]\n",
    "\n",
    "            # First Pass\n",
    "            grad_weight = 1e4\n",
    "            style_weight = 1e4\n",
    "            content_weight = 1\n",
    "            tv_weight = 1e-6\n",
    "\n",
    "            canvas_mask = make_canvas_mask(x_start, y_start, target_img, mask_img)\n",
    "            canvas_mask = numpy2tensor(canvas_mask, self.device)\n",
    "            canvas_mask = canvas_mask.squeeze(0).repeat(3, 1).view(3, self.ts, self.ts).unsqueeze(0)\n",
    "\n",
    "            gt_gradient = compute_gt_gradient(x_start, y_start, source_img, target_img, mask_img, self.device)\n",
    "\n",
    "            source_img = torch.from_numpy(source_img).unsqueeze(0).transpose(1, 3).transpose(2, 3).float().to(self.device)\n",
    "            target_img = torch.from_numpy(target_img).unsqueeze(0).transpose(1, 3).transpose(2, 3).float().to(self.device)\n",
    "            input_img = torch.randn(target_img.shape).to(self.device)\n",
    "\n",
    "            mask_img = numpy2tensor(mask_img, self.device)\n",
    "            mask_img = mask_img.squeeze(0).repeat(3, 1).view(3, ss, ss).unsqueeze(0)\n",
    "\n",
    "            def get_input_optimizer(input_img):\n",
    "                optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "                return optimizer\n",
    "            optimizer = get_input_optimizer(input_img)\n",
    "\n",
    "            mse = torch.nn.MSELoss()\n",
    "\n",
    "            mean_shift = MeanShift(self.device)\n",
    "            vgg = Vgg16().to(self.device)\n",
    "\n",
    "            run = [0]\n",
    "            while run[0] < self.num_steps1:\n",
    "\n",
    "                def closure():\n",
    "                    blend_img = torch.zeros(target_img.shape).to(self.device)\n",
    "                    blend_img = input_img*canvas_mask + target_img*(canvas_mask-1)*(-1)\n",
    "\n",
    "                    pred_gradient = laplacian_filter_tensor(blend_img, self.device)\n",
    "\n",
    "                    grad_loss = 0\n",
    "                    for c in range(len(pred_gradient)):\n",
    "                        grad_loss += mse(pred_gradient[c], gt_gradient[c])\n",
    "                    grad_loss /= len(pred_gradient)\n",
    "                    grad_loss *= grad_weight\n",
    "\n",
    "                    target_features_style = vgg(mean_shift(target_img))\n",
    "                    target_gram_style = [gram_matrix(y) for y in target_features_style]\n",
    "\n",
    "                    blend_features_style = vgg(mean_shift(input_img))\n",
    "                    blend_gram_style = [gram_matrix(y) for y in blend_features_style]\n",
    "\n",
    "                    style_loss = 0\n",
    "                    for layer in range(len(blend_gram_style)):\n",
    "                        style_loss += mse(blend_gram_style[layer], target_gram_style[layer])\n",
    "                    style_loss /= len(blend_gram_style)\n",
    "                    style_loss *= style_weight\n",
    "\n",
    "                    blend_obj = blend_img[:, :, int(x_start-source_img.shape[2]*0.5):int(x_start+source_img.shape[2]*0.5), int(\n",
    "                        y_start-source_img.shape[3]*0.5):int(y_start+source_img.shape[3]*0.5)]\n",
    "                    source_object_features = vgg(mean_shift(source_img*mask_img))\n",
    "                    blend_object_features = vgg(mean_shift(blend_obj*mask_img))\n",
    "                    content_loss = content_weight * mse(blend_object_features.relu2_2, source_object_features.relu2_2)\n",
    "                    content_loss *= content_weight\n",
    "\n",
    "                    tv_loss = torch.sum(torch.abs(blend_img[:, :, :, :-1] - blend_img[:, :, :, 1:])) + \\\n",
    "                        torch.sum(torch.abs(blend_img[:, :, :-1, :] - blend_img[:, :, 1:, :]))\n",
    "                    tv_loss *= tv_weight\n",
    "\n",
    "                    loss = grad_loss + style_loss + content_loss + tv_loss\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "\n",
    "                    run[0] += 1\n",
    "                    return loss\n",
    "\n",
    "                optimizer.step(closure)\n",
    "\n",
    "            input_img.data.clamp_(0, 255)\n",
    "\n",
    "            blend_img = torch.zeros(target_img.shape).to(self.device)\n",
    "            blend_img = input_img*canvas_mask + target_img*(canvas_mask-1)*(-1)\n",
    "            blend_img_np = blend_img.transpose(1, 3).transpose(1, 2).cpu().data.numpy()[0]\n",
    "\n",
    "            if self.num_steps1 > 0:\n",
    "                first_pass_img_file = os.path.join(self.outputImagesDir, split, f'{file_save_index}.png')\n",
    "                imsave(first_pass_img_file, blend_img_np.astype(np.uint8))\n",
    "\n",
    "                str_metadata = f\"{file_save_index}.png {j+1}/{no_pests} {target_filename} {source_filename} {(y_start, x_start)}\\n\"\n",
    "\n",
    "                metadata_path = os.path.join(self.outputMetadataDir, split, \"metadata.txt\")\n",
    "                with open(metadata_path, 'a') as f:\n",
    "                    f.write(str_metadata)\n",
    "                    f.close()\n",
    "\n",
    "                label_path = os.path.join(self.outputLabelsDir, split, f'{file_save_index}.txt')\n",
    "                with open(label_path, 'a') as f:\n",
    "                    f.write(f\"{pest_class_id} {y_start/self.ts} {x_start/self.ts} {ss/self.ts} {ss/self.ts}\\n\")\n",
    "\n",
    "            target_img = np.array(Image.open(first_pass_img_file).convert('RGB').resize((self.ts, self.ts)))\n",
    "\n",
    "\n",
    "N_generate = 1\n",
    "\n",
    "deep_image_blending = DeepImageBlending(outputImagesDir='D:/git/PestVisionChallenge/synthetic_data/test_dib/images',\n",
    "                                        outputLabelsDir='D:/git/PestVisionChallenge/synthetic_data/test_dib/labels',\n",
    "                                        outputMetadataDir='D:/git/PestVisionChallenge/synthetic_data/test_dib/metadata',\n",
    "                                        device='cuda:0',\n",
    "                                        max_pests_per_image=5,\n",
    "                                        target_image_size=512,\n",
    "                                        source_image_range_big=source_range_big,\n",
    "                                        source_image_range_small=source_range_small,\n",
    "                                        num_steps1=1000)\n",
    "\n",
    "for i in tqdm(range(N_generate)):\n",
    "   \n",
    "   split_prob = np.random.rand()\n",
    "\n",
    "   if split_prob < 0.65:\n",
    "      split = 'train'\n",
    "   elif split_prob < 0.80:\n",
    "      split = 'val'\n",
    "   else: \n",
    "      split = 'test'\n",
    "\n",
    "   foreground_dataset = foreground_datasets[split]\n",
    "   background_dataset = background_datasets[split]\n",
    "\n",
    "   N_foreground, N_background = len(foreground_dataset), len(background_dataset)\n",
    "\n",
    "   deep_image_blending.generate_blended_image(foreground_dataset = foreground_dataset,\n",
    "                                              background_dataset = background_dataset,\n",
    "                                              N_foreground = N_foreground,\n",
    "                                              N_background = N_background,\n",
    "                                              split = split,\n",
    "                                              file_save_index=i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Polynomial: -5*x**3 - 2*x**2 - x + 1\n",
      "Invertible: True\n",
      "The inverse function is not single-valued, indicating the polynomial is not invertible.\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_random_polynomial(degree, variable, coefficient_range):\n",
    "    \"\"\"\n",
    "    Generate a random polynomial of a given degree.\n",
    "    \n",
    "    Parameters:\n",
    "    degree (int): Degree of the polynomial\n",
    "    variable (sympy.Symbol): The variable for the polynomial\n",
    "    coefficient_range (tuple): Range of coefficients as (min, max)\n",
    "    \n",
    "    Returns:\n",
    "    sympy.Poly: The generated polynomial\n",
    "    \"\"\"\n",
    "    coefficients = [random.randint(*coefficient_range) for _ in range(degree + 1)]\n",
    "    polynomial = sum(coeff * variable**i for i, coeff in enumerate(coefficients))\n",
    "    return sp.Poly(polynomial, variable)\n",
    "\n",
    "def is_invertible(polynomial):\n",
    "    \"\"\"\n",
    "    Check if the polynomial is invertible.\n",
    "    \n",
    "    Parameters:\n",
    "    polynomial (sympy.Poly): The polynomial to check\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if invertible, False otherwise\n",
    "    \"\"\"\n",
    "    # A polynomial is invertible if its constant term is non-zero\n",
    "    return polynomial.LC() != 0\n",
    "\n",
    "def plot_polynomial_and_inverse(polynomial):\n",
    "    \"\"\"\n",
    "    Plot the polynomial and its inverse function.\n",
    "    \n",
    "    Parameters:\n",
    "    polynomial (sympy.Poly): The polynomial to plot\n",
    "    \"\"\"\n",
    "    x = sp.symbols('x')\n",
    "    y = sp.symbols('y')\n",
    "    func = polynomial.as_expr()\n",
    "    \n",
    "    # Solve for y in terms of x\n",
    "    inverse_func = sp.solve(sp.Eq(func, y), x)\n",
    "    \n",
    "    # Generate data points for plotting\n",
    "    x_vals = np.linspace(-10, 10, 400)\n",
    "    y_vals = np.array([func.subs(x, val) for val in x_vals], dtype=float)\n",
    "    \n",
    "    # Ensure the inverse function is valid and single-valued\n",
    "    if len(inverse_func) == 1:\n",
    "        inverse_func = inverse_func[0]\n",
    "        y_vals_inverse = np.linspace(-10, 10, 400)\n",
    "        x_vals_inverse = np.array([inverse_func.subs(y, val) for val in y_vals_inverse], dtype=float)\n",
    "        \n",
    "        # Plot the polynomial and its inverse\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(x_vals, y_vals, label=f'{func}')\n",
    "        plt.plot(y_vals_inverse, x_vals_inverse, label=f'Inverse of {func}', linestyle='dashed')\n",
    "        plt.plot(x_vals, x_vals, label='y = x', linestyle='dotted', color='gray')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.legend()\n",
    "        plt.title('Polynomial and its Inverse')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"The inverse function is not single-valued, indicating the polynomial is not invertible.\")\n",
    "\n",
    "# Example usage\n",
    "x = sp.symbols('x')\n",
    "degree = 3\n",
    "coefficient_range = (-10, 10)\n",
    "\n",
    "# Generate a random polynomial\n",
    "poly = generate_random_polynomial(degree, x, coefficient_range)\n",
    "\n",
    "# Check if the polynomial is invertible\n",
    "invertible = is_invertible(poly)\n",
    "\n",
    "print(f\"Random Polynomial: {poly.as_expr()}\")\n",
    "print(f\"Invertible: {invertible}\")\n",
    "\n",
    "if invertible:\n",
    "    plot_polynomial_and_inverse(poly)\n",
    "else:\n",
    "    print(\"The polynomial is not invertible.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\jesli\\anaconda3\\envs\\pv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jesli\\anaconda3\\envs\\pv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "  0%|          | 0/5 [00:34<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m\n\u001b[0;32m     28\u001b[0m background_dataset \u001b[38;5;241m=\u001b[39m background_datasets[split]\n\u001b[0;32m     30\u001b[0m N_foreground, N_background \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(foreground_dataset), \u001b[38;5;28mlen\u001b[39m(background_dataset)\n\u001b[1;32m---> 32\u001b[0m deep_image_blending\u001b[38;5;241m.\u001b[39mgenerate_blended_image(foreground_dataset \u001b[38;5;241m=\u001b[39m foreground_dataset,\n\u001b[0;32m     33\u001b[0m                                            background_dataset \u001b[38;5;241m=\u001b[39m background_dataset,\n\u001b[0;32m     34\u001b[0m                                            N_foreground \u001b[38;5;241m=\u001b[39m N_foreground,\n\u001b[0;32m     35\u001b[0m                                            N_background \u001b[38;5;241m=\u001b[39m N_background,\n\u001b[0;32m     36\u001b[0m                                            split \u001b[38;5;241m=\u001b[39m split,\n\u001b[0;32m     37\u001b[0m                                            file_save_index\u001b[38;5;241m=\u001b[39mi)\n",
      "File \u001b[1;32md:\\git\\PestVisionChallenge\\synthetic_data_generation\\pest_blending.py:279\u001b[0m, in \u001b[0;36mDeepImageBlending.generate_blended_image\u001b[1;34m(self, foreground_dataset, N_foreground, background_dataset, N_background, split, file_save_index)\u001b[0m\n\u001b[0;32m    276\u001b[0m         run[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m--> 279\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep(closure)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# clamp the pixels range into 0 ~ 255\u001b[39;00m\n\u001b[0;32m    282\u001b[0m input_img\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jesli\\anaconda3\\envs\\pv\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jesli\\anaconda3\\envs\\pv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jesli\\anaconda3\\envs\\pv\\Lib\\site-packages\\torch\\optim\\lbfgs.py:445\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_iter \u001b[38;5;241m!=\u001b[39m max_iter:\n\u001b[0;32m    441\u001b[0m     \u001b[38;5;66;03m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;66;03m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;66;03m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[1;32m--> 445\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(closure())\n\u001b[0;32m    446\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[0;32m    447\u001b[0m     opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from pest_blending import DeepImageBlending\n",
    "\n",
    "N_generate = 5\n",
    "\n",
    "#change the paths appropriately\n",
    "deep_image_blending = DeepImageBlending(outputImagesDir='D:/git/PestVisionChallenge/synthetic_data/test_dib/images',\n",
    "                                        outputLabelsDir='D:/git/PestVisionChallenge/synthetic_data/test_dib/labels',\n",
    "                                        outputMetadataDir='D:/git/PestVisionChallenge/synthetic_data/test_dib/metadata',\n",
    "                                        device='cuda:0',\n",
    "                                        max_pests_per_image=5,\n",
    "                                        target_image_size=512,\n",
    "                                        source_image_range_big=source_range_big,\n",
    "                                        source_image_range_small=source_range_small,\n",
    "                                        num_steps1=1000)\n",
    "\n",
    "for i in tqdm(range(N_generate)):\n",
    "   \n",
    "   split_prob = np.random.rand()\n",
    "\n",
    "   if split_prob < 0.65:\n",
    "      split = 'train'\n",
    "   elif split_prob < 0.80:\n",
    "      split = 'val'\n",
    "   else: \n",
    "      split = 'test'\n",
    "\n",
    "   foreground_dataset = foreground_datasets[split]\n",
    "   background_dataset = background_datasets[split]\n",
    "\n",
    "   N_foreground, N_background = len(foreground_dataset), len(background_dataset)\n",
    "\n",
    "   deep_image_blending.generate_blended_image(foreground_dataset = foreground_dataset,\n",
    "                                              background_dataset = background_dataset,\n",
    "                                              N_foreground = N_foreground,\n",
    "                                              N_background = N_background,\n",
    "                                              split = split,\n",
    "                                              file_save_index=i)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pest_blending import LibcomImageHarmonization\n",
    "\n",
    "N_generate = 10\n",
    "\n",
    "libcom_image_harmonization = LibcomImageHarmonization(outputImagesDir='/home/siddhibrahmbhatt/code_siddhi/PestVisionChallengeChallenge/synthetic_data_generation/test_ih/images',\n",
    "                                                      outputLabelsDir='/home/siddhibrahmbhatt/code_siddhi/PestVisionChallenge/synthetic_data_generation/test_ih/labels',\n",
    "                                                      outputMetadataDir='/home/siddhibrahmbhatt/code_siddhi/PestVisionChallenge/synthetic_data_generation/test_ih/metadata',\n",
    "                                                      outputTempDir='/home/siddhibrahmbhatt/code_siddhi/PestVisionChallenge/synthetic_data_generation/test_ih/temp_libcom',\n",
    "                                                      device=0,\n",
    "                                                      max_pests_per_image=5,\n",
    "                                                      target_image_size=512,\n",
    "                                                      source_image_range_big=source_range_big,\n",
    "                                                      source_image_range_small=source_range_small,\n",
    "                                                      model_type = \"PCTNet\")\n",
    "\n",
    "for i in tqdm(range(N_generate)):\n",
    "   \n",
    "   split_prob = np.random.rand()\n",
    "\n",
    "   if split_prob < 0.65:\n",
    "      split = 'train'\n",
    "   elif split_prob < 0.80:\n",
    "      split = 'val'\n",
    "   else: \n",
    "      split = 'test'\n",
    "\n",
    "   foreground_dataset = foreground_datasets[split]\n",
    "   background_dataset = background_datasets[split]\n",
    "\n",
    "   N_foreground, N_background = len(foreground_dataset), len(background_dataset)\n",
    "\n",
    "   libcom_image_harmonization.generate_blended_image(foreground_dataset = foreground_dataset,\n",
    "                                                        background_dataset = background_dataset,\n",
    "                                                        N_foreground = N_foreground,\n",
    "                                                        N_background = N_background,\n",
    "                                                        split = split,\n",
    "                                                        file_save_index=i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pestvision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
